# -*- coding: utf-8 -*-
"""Diabetes Prediction: EXPOSYS DATALABS INTERNSHIP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uROyfsorKgEqUANOZ4Kc2FXpncBFSoc8

**Importing the Dependencies**
"""

import numpy as np              # to convert out data into a format suitable to feed our classification model
import pandas as pd             # to read our data from a CSV file and manipulate it for further use
import matplotlib.pyplot as plt # for visualizations
import seaborn as sns           # for visualizations
import math
from matplotlib import pyplot
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from yellowbrick.model_selection import FeatureImportances
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectPercentile, f_classif

pd.__version__

np.__version__

sns.__version__

""" **Data Collection and Analysis**"""

# loading the diabetes dataset to a pandas DataFrame
diabetes_dataset = pd.read_csv('/content/diabetes.csv')

type(diabetes_dataset)

pd.read_csv?

# printing the first 5 rows of the dataset
diabetes_dataset.head()

"""The following features have been provided to help us predict whether a person is diabetic or not:

*Pregnancies*: Number of times pregnant

*Glucose*: Plasma glucose concentration over 2 hours in an oral glucose tolerance test

*BloodPressure*: Diastolic blood pressure (mm Hg)

*SkinThickness*: Triceps skin fold thickness (mm)

*Insulin*: 2-Hour serum insulin (mu U/ml)
BMI: Body mass index (weight in kg/(height in m)2)

*DiabetesPedigreeFunction*: Diabetes pedigree function (a function which scores likelihood of diabetes based on family history)

*Age*: Age (years)

*Outcome*: Class variable (0 if non-diabetic, 1 if diabetic)
"""

# number of rows and columns in this dataset
diabetes_dataset.shape

# getting some informations about the dataset
diabetes_dataset.info()

# number of missing values in each column
diabetes_dataset.isnull().sum()

"""**Data Analysis and Visulaization**"""

# getting the statistical measures of the data
diabetes_dataset.describe()

diabetes_dataset.boxplot(figsize=(20,5))

plt.figure(figsize=(13,5))
sns.boxplot(data=diabetes_dataset, orient='h')
plt.show

sns.scatterplot(x=diabetes_dataset['Glucose'],y=diabetes_dataset['Outcome'])
plt.show()

diabetes_dataset.replace()

# separating the data and labels
X=diabetes_dataset.drop('Outcome',axis=1)

Y=diabetes_dataset['Outcome']

X.describe()

X.replace(to_replace=0,value=X.mean(),inplace=True)

X.describe()

X.boxplot(figsize=(20,5))
plt.show()

X

Y

feature_names = ["Pregnancies", "Glucose", "BloodPressure",	"SkinThickness","Insulin","BMI","DiabetesPedigreeFunction",	"Age"]
diabetes_dataset = pd.DataFrame(X,columns = feature_names)
diabetes_dataset['Outcome'] = Y

# distribution of Pregnancies value 
sns.set()
plt.figure(figsize=(6,6))
sns.displot(diabetes_dataset['Pregnancies'])
plt.title('Pregnancies Distribution')
plt.show()

# Glucose for diabetic
plt.figure(figsize=(6,6))
sns.displot(diabetes_dataset['Glucose'])
plt.title('Glucose Distribution')
plt.show()

# BloodPressure for diabetic
plt.figure(figsize=(6,6))
sns.histplot(diabetes_dataset['BloodPressure'])
plt.title('BloodPressure Distribution')
plt.show()

# SkinThickness for diabetic
plt.figure(figsize=(6,6))
sns.displot(diabetes_dataset['SkinThickness'])
plt.title('SkinThickness')
plt.show()

# Insulin for diabetic
plt.figure(figsize=(6,6))
sns.displot(diabetes_dataset['Insulin'])
plt.title('Insulin')
plt.show()

# DiabetesPedigreeFunction for diabetic
plt.figure(figsize=(6,6))
sns.displot(diabetes_dataset['DiabetesPedigreeFunction'])
plt.title('DiabetesPedigreeFunction')
plt.show()

# BMI for diabetic
plt.figure(figsize=(6,6))
sns.displot(diabetes_dataset['BMI'])
plt.title('BMI')
plt.show()

# Age for diabetic
plt.figure(figsize=(6,6))
sns.displot(diabetes_dataset['Age'])
plt.title('Age')
plt.show()

"""*Understanding the correlation between various features in the dataset*

*1.Positive Correlation*

*2.Negative Correlation* 
"""

correlation = diabetes_dataset.corr()

# constructing a heatmap to understand the correlation
plt.figure(figsize=(10,10))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':8}, cmap='Greens')

"""Here in the above heatmap, brighter colors indicate more correlation.

As we can see from the table and the heatmap, **glucose levels, age, BMI and number of pregnancies** all have signicant correlation with the outcome variable.

Also notice the correlation between pairs of features, like age and pregnancies, or insulin and skin thickness.
"""

diabetes_dataset['Outcome'].value_counts()

"""*0 -->Non-Diabetes*

*1 -->Diabetes*
"""

diabetes_dataset.groupby('Outcome').mean()

# number of values for each outcome
sns.catplot(x='Outcome', data = diabetes_dataset, kind = 'count')

"""The above plot shows how many people in the dataset are diabetic and how many are not.

**Data Standardisation**
"""

scaler = StandardScaler()

scaler.fit(X)

standardized_data = scaler.transform(X)

print(standardized_data)

X = standardized_data
Y = diabetes_dataset['Outcome']

print(X)
print(Y)

# feature names as a list
col= diabetes_dataset.columns
print(col)

y = diabetes_dataset.Outcome
list = ['Outcome']
x = diabetes_dataset.drop(list, axis = 1)
x.head()

"""**Train Test Split**"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, stratify=Y, random_state=2)

X_test

print(X.shape, X_train.shape, X_test.shape)

"""**Model Training --> (1) KNN Classifier in Sklearn**"""

classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)

classifier.fit(X_train, Y_train)

from sklearn.inspection import permutation_importance
# perform permutation importance
results = permutation_importance(classifier, X_train, Y_train, scoring='accuracy')
# get importance
importance = results.importances_mean
# summarize feature importance
for i,v in enumerate(importance):
	print('Feature: %0d, Score: %.5f' % (i,v))
# plot feature importance
plt.bar([x for x in range(len(importance))], importance)
plt.show()

Y_pred = classifier.predict(X_test)

cm = confusion_matrix(Y_test, Y_pred)

print(cm)

"""***Model Evaluation***"""

print("Correct:",sum(Y_pred == Y_test))

print("Incorrect:",sum(Y_pred != Y_test))

print("Accuracy:",sum(Y_pred == Y_test)/len(Y_pred))

"""**Model Training --> (2) Support Vector Machine**"""

X_indices = np.arange(X.shape[-1])
selector = SelectPercentile(f_classif, percentile=10)
selector.fit(X_train, Y_train)

# Compare to the weights of an SVM
classifier = svm.SVC(kernel='linear')
classifier.fit(X_train, Y_train)

svm_weights = (classifier.coef_ ** 2).sum(axis=0)
svm_weights /= svm_weights.max()

plt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight', color='r')

classifier_selected = svm.SVC(kernel='linear')
classifier_selected.fit(selector.transform(X_train), Y_train)

svm_weights_selected = (classifier_selected.coef_ ** 2).sum(axis=0)
svm_weights_selected /= svm_weights_selected.max()

plt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,
        width=.2, label='SVM weights after selection', color='b')


plt.title("Comparing feature selection")
plt.xlabel('Feature numbers')
plt.yticks(())
plt.axis('tight')
plt.legend(loc='upper right')
plt.show()

#training the support vector Machine Classifier
classifier.fit(X_train, Y_train)

"""*Model Evaluation*"""

# accuracy score on the training data
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data :', training_data_accuracy)

# accuracy score on the test data
X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data :', test_data_accuracy)

"""**Model training -->(3) Logistic Regression**"""

model = LogisticRegression()

#training the Logistic Regression model with training data
model.fit(X_train, Y_train)

Y_pred=model.predict(X_test)

Y_pred

Y_test

accuracy_score(Y_test,Y_pred)

w = model.coef_[0]
feature_importance = pd.DataFrame(feature_names, columns = ["feature"])
feature_importance["importance"] = pow(math.e, w)
feature_importance = feature_importance.sort_values(by = ["importance"], ascending=False)
 

ax = feature_importance.plot.barh(x='feature', y='importance')
plt.show()

"""**Model Training --> (4) Decision Tree**"""

model = DecisionTreeClassifier()

model.fit(X_train,Y_train)

Y_pred = model.predict(X_test)

Y_pred

Y_test

accuracy_score(Y_test,Y_pred)

model.feature_importances_

diabetes_dataset = pd.DataFrame({'Feature_names' :x.columns,'Importances' : model.feature_importances_})
diabetes_dataset

diabetes_dataset_1 = diabetes_dataset.sort_values(by='Importances',ascending=False)
diabetes_dataset_1

ax = plt.barh(diabetes_dataset_1['Feature_names'], diabetes_dataset_1['Importances'])
plt.show()

"""

**Model Training --> (5) Random Forest Classifier**"""

model = RandomForestClassifier()

model.fit(X_train, Y_train)

feature_imp = pd.Series(model.feature_importances_,index=feature_names).sort_values(ascending=False)
feature_imp

sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()

"""*Model Evaluation*"""

# accuracy on test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy : ', test_data_accuracy)

"""**Model Training --> (6) Naive Bayes**"""

classifier = GaussianNB()
classifier.fit(X_train, Y_train)

Y_pred = classifier.predict(X_test)

cm = confusion_matrix(Y_pred, Y_test)

print(cm)

"""*Model Evaluation*"""

print("Correct:",sum(Y_pred == Y_test))

print("Incorrect:",sum(Y_pred != Y_test))

print("Accuracy:",sum(Y_pred == Y_test)/len(Y_pred))

"""The naive bayes classifers don't offer an intrinsic method to evaluate feature importances. Naïve Bayes methods work by determining the conditional and unconditional probabilities associated with the features and predict the class with the highest probability. Thus, there are no coefficients computed or associated with the features we used to train the model.

One of the methods that can be applied post-hoc to analyze the model after it has been trained, is the Permutation Importance and it, conveniently, has also been implemented in scikit-learn.

Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. The permutation_importance function calculates the feature importance of estimators for a given dataset. The n_repeats parameter sets the number of times a feature is randomly shuffled and returns a sample of feature importances.

**Making a Predictive System**
"""

input_data = (5,166,72,19,175,25.8,0.587,51)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if(prediction[0] == 0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')

"""**Result:** The dataset consists of 768 records and 9 columns.

The **support vector machine model** is able to classify patients as diabetic or not with an accuracy of **77.27%**


The **Logistic Regression model** is able to classify patients as diabetic or not with an accuracy of **75.97% **

The **Random forest classifier model** is able to classify patients as diabetic or not with an accuracy of **74.67%** 

The **naive Bayes model** is able to classify patients as diabetic or not with an accuracy of **72.73%**


The **KNN model** is able to classify patients as diabetic or not with an accuracy of **72.07%**


The **Decision tree model** is able to classify patients as diabetic or not with an accuracy of 68.18%


Glucose level, BMI, pregnancies and diabetes pedigree function have significant influence on the model, specially glucose level and BMI. It is good to see our machine learning model match what we have been hearing from doctors our entire lives!



"""